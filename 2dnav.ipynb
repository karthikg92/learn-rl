{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "play-karthik.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOyti6Sq0cEtiEgY+cM2V0U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karthikg92/learn-rl/blob/main/2dnav.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "KxCz3kX7enNm"
      },
      "outputs": [],
      "source": [
        "import numpy as np                        # numpy functionality\n",
        "from itertools import count\n",
        "from collections import deque\n",
        "import random\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "from torch.distributions import Normal\n",
        "import torch.autograd as autograd         # computation graph\n",
        "from torch import Tensor                  # tensor node in the computation graph\n",
        "import torch.nn as nn                     # neural networks\n",
        "import torch.nn.functional as F           # layers, activations and more\n",
        "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, \n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Environment for 2d navigation\n",
        "\n",
        "Objective: move a robot in a 2d plane from a start location to a goal\n",
        "\n",
        "State: 4 dimensional vector (x position, y position, goal x position, goal y position). The robot lives in [0, size]^2 world\n",
        "\n",
        "Action: velocity vector (v_x, v_y) with fixed speed of max_speed\n",
        "\n",
        "Rewards: -5 if out of bounds, 10 if it reaches the goal, -1 for all other cases\n",
        "\"\"\"\n",
        "\n",
        "class nav2d():\n",
        "\n",
        "    def __init__(self, size=1):\n",
        "        # The size of the square grid [0, self.size]^2\n",
        "        self.size = size  \n",
        "\n",
        "        # Initialize the state and the goal\n",
        "        self.state = self.reset()\n",
        "\n",
        "        # parameters\n",
        "        self.max_speed= 0.05 # max robot speed\n",
        "        self.goal_dist = 0.1 # threshold within which robot reaches goal\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "\n",
        "        # start and end location drawn uniformly at random\n",
        "        # self.state = [\n",
        "        #     np.random.rand() * self.size, # x\n",
        "        #     np.random.rand() * self.size, # y\n",
        "        #     np.random.rand() * self.size, # goal_x\n",
        "        #     np.random.rand() * self.size  # goal_y\n",
        "        # ]\n",
        "\n",
        "        # deterministic start and end location\n",
        "        self.state = [0.1, 0.1, 0.6, 0.8, np.sqrt((0.1 - 0.6)**2 + (0.1 - 0.8)**2 )]\n",
        "\n",
        "        # state = [x_loc, y_loc, goal x_loc, goal y_loc, dist to goal]\n",
        "\n",
        "        return self.state\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "      \"\"\"\n",
        "      action is a list/ array of length 2\n",
        "      action[0]:  x velocity\n",
        "      action[1]:  y velocity\n",
        "      \"\"\"\n",
        "\n",
        "      # robot kinematics\n",
        "      self.state[0] += self.max_speed * action[0]\n",
        "      self.state[1] += self.max_speed * action[1]\n",
        "\n",
        "      # computing rewards basedon distance to goal\n",
        "      dist2goal = np.sqrt( (self.state[0] - self.state[2]) **2 + (self.state[1] - self.state[3]) **2 )\n",
        "\n",
        "      self.state[4] = dist2goal\n",
        "\n",
        "      # default info\n",
        "      info = {'reached_goal': False}\n",
        "\n",
        "      if dist2goal < self.goal_dist:\n",
        "        # robot within goal\n",
        "        reward = 10\n",
        "        done = 1\n",
        "        info['reached_goal'] = True\n",
        "      elif self.state[0] < 0 or self.state[0] > self.size or self.state[1] < 0 or self.state[1] > self.size:\n",
        "        # robot out of bounds of the 2-d environment\n",
        "        reward = -5\n",
        "        done = 0\n",
        "      else:\n",
        "        # robot moving legally within the 2d world\n",
        "        reward = -2 * dist2goal\n",
        "        done = 0\n",
        "      \n",
        "\n",
        "      return self.state, reward, done, info\n",
        "\n",
        "    # def render(self):\n",
        "    #   plt.scatter(self.state[0], self.state[1], c='black')\n",
        "    #   plt.scatter(self.state[2], self.state[3], c='green')\n",
        "    #   plt.xlim([0, self.size])\n",
        "    #   plt.ylim([0, self.size])\n",
        "\n"
      ],
      "metadata": {
        "id": "gFHRTLHnfdr2"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input=4, output=2):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(input, 64)\n",
        "        self.fc2 = nn.Linear(64, 256)\n",
        "        self.fc_mu = nn.Linear(256, output)\n",
        "        self.fc_std = nn.Linear(256, output)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.softplus = nn.Softplus()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        mu = 2 * self.tanh(self.fc_mu(x))\n",
        "        std = self.softplus(self.fc_std(x)) + 1e-3\n",
        "        return mu, std\n",
        "\n",
        "    def select_action(self, state):\n",
        "        with torch.no_grad():\n",
        "            mu, std = self.forward(state)\n",
        "            n = Normal(mu, std)\n",
        "            action = n.sample()\n",
        "            action = torch.clip(action, min=-1, max=1)\n",
        "            action = action / torch.norm(action)\n",
        "        action = action.tolist()[0]\n",
        "        return action\n",
        "\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, input=4, output=1):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(input, 64)\n",
        "        self.fc2 = nn.Linear(64, 256)\n",
        "        self.fc3 = nn.Linear(256, output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Memory(object):\n",
        "    def __init__(self, memory_size: int) -> None:\n",
        "        self.memory_size = memory_size\n",
        "        self.buffer = deque(maxlen=self.memory_size)\n",
        "\n",
        "    def add(self, experience) -> None:\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def sample(self, batch_size: int, continuous: bool = True):\n",
        "        if batch_size > len(self.buffer):\n",
        "            batch_size = len(self.buffer)\n",
        "        if continuous:\n",
        "            rand = random.randint(0, len(self.buffer) - batch_size)\n",
        "            return [self.buffer[i] for i in range(rand, rand + batch_size)]\n",
        "        else:\n",
        "            indexes = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)\n",
        "            return [self.buffer[i] for i in indexes]\n",
        "\n",
        "    def clear(self):\n",
        "        self.buffer.clear()\n",
        "\n",
        "\n",
        "env = nav2d()\n",
        "\n",
        "state_dim = 5\n",
        "action_dim = 2\n",
        "policy = PolicyNetwork(input=state_dim, output=action_dim).to(device)\n",
        "value = ValueNetwork(input=state_dim).to(device)\n",
        "optim = torch.optim.Adam(policy.parameters(), lr=1e-5)\n",
        "value_optim = torch.optim.Adam(value.parameters(), lr=3e-5)\n",
        "gamma = 0.99\n",
        "episode_length = 20\n",
        "max_epochs = 1000000\n",
        "epochs_per_batch = 1\n",
        "batch_size = episode_length * epochs_per_batch\n",
        "memory = Memory(batch_size)\n",
        "steps = 0\n",
        "\n",
        "saved_rewards = []\n",
        "reached_goal = []\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "\n",
        "    state = env.reset()\n",
        "    episode_reward = 0\n",
        "  \n",
        "    time_step = 0\n",
        "    while True:\n",
        "\n",
        "        time_step += 1\n",
        "\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        action = policy.select_action(state_tensor)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        episode_reward += reward\n",
        "\n",
        "        memory.add((state, next_state, action, reward, done))\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        # dist to goal:\n",
        "        #dist_to_goal = np.sqrt((state[0] - state[2])**2 +(state[1] - state[3])**2)\n",
        "\n",
        "\n",
        "        if done:\n",
        "          if info['reached_goal']:\n",
        "            reached_goal.append(1)\n",
        "          else:\n",
        "            reached_goal.append(0)\n",
        "          break\n",
        "\n",
        "        if time_step == episode_length:\n",
        "          reached_goal.append(0)\n",
        "          break\n",
        "        \n",
        "\n",
        "    if epoch % epochs_per_batch == 0:\n",
        "\n",
        "        experiences = memory.sample(batch_size)\n",
        "        batch_state, batch_next_state, batch_action, batch_reward, batch_done = zip(*experiences)\n",
        "        batch_state = torch.FloatTensor(batch_state).to(device)\n",
        "        batch_next_state = torch.FloatTensor(batch_next_state).to(device)\n",
        "        batch_action = torch.FloatTensor(batch_action).unsqueeze(1).to(device)\n",
        "        batch_reward = torch.FloatTensor(batch_reward).unsqueeze(1).to(device)\n",
        "        batch_done = torch.FloatTensor(batch_done).unsqueeze(1).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            value_target = batch_reward + gamma * (1 - batch_done) * value(batch_next_state)\n",
        "            advantage = value_target - value(batch_state)\n",
        "\n",
        "        mu, std = policy(batch_state)\n",
        "        n = Normal(mu, std)\n",
        "        log_prob = n.log_prob(batch_action)\n",
        "        loss = - log_prob * advantage\n",
        "        loss = loss.mean()\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        value_loss = F.mse_loss(value_target, value(batch_state))\n",
        "        value_optim.zero_grad()\n",
        "        value_loss.backward()\n",
        "        value_optim.step()\n",
        "\n",
        "        memory.clear()\n",
        "      \n",
        "        \n",
        "    saved_rewards.append(episode_reward)\n",
        "    if epoch % 1000 == 0:\n",
        "      if len(saved_rewards)>1000:\n",
        "        print('Epoch:{}, average episode rewards of last 1000 epochs is {}'.format(epoch, np.mean(saved_rewards[-1000:])))\n",
        "        print(\"reached_goal out of last 1000 episodes: \", np.sum(reached_goal[-1000:]))\n",
        "\n",
        "    #print('\\n\\n\\n')\n"
      ],
      "metadata": {
        "id": "VXnJ9CF3EZPC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "outputId": "6dc80f8a-cce2-4aa9-ac8e-d7e27d653c60"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1000, average episode rewards of last 1000 epochs is -46.9702374746413\n",
            "reached_goal out of last 1000 episodes:  0\n",
            "Epoch:2000, average episode rewards of last 1000 epochs is -38.19837844357277\n",
            "reached_goal out of last 1000 episodes:  0\n",
            "Epoch:3000, average episode rewards of last 1000 epochs is -37.263997375815016\n",
            "reached_goal out of last 1000 episodes:  0\n",
            "Epoch:4000, average episode rewards of last 1000 epochs is -33.91256131178801\n",
            "reached_goal out of last 1000 episodes:  0\n",
            "Epoch:5000, average episode rewards of last 1000 epochs is -31.979222308713446\n",
            "reached_goal out of last 1000 episodes:  0\n",
            "Epoch:6000, average episode rewards of last 1000 epochs is -31.836076101737333\n",
            "reached_goal out of last 1000 episodes:  0\n",
            "Epoch:7000, average episode rewards of last 1000 epochs is -32.32426883683571\n",
            "reached_goal out of last 1000 episodes:  0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-7aea493c65ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mstate_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-7aea493c65ae>\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8yCI-vO72lpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "78_3pUWVNlhh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}